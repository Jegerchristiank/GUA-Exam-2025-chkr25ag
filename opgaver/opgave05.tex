\section*{Assignment 05}
\addcontentsline{toc}{section}{Assignment 05}

\subsection*{Onboarding, feedback loops, and moderation}
I treat onboarding as a blend of storytelling and friction-testing. New users first encounter a grounded value proposition in the landing flow, then they are nudged through a guided tour that shows the key interactions (posting, booking, matching). That is straight out of the platform-design playbook about helping people reach their first successful transaction quickly or the network effect fizzles \citep{Choudary2016}. To make the steps crystal clear in English, I break onboarding into phases: (1) pre-signup nudges (a short video plus social proof), (2) profile setup with pre-filled suggestions, and (3) a ``first mission'' checklist that awards badges once someone has touched the core features. We also pair newcomers with mentors or automated prompts so there is always a response in their inbox within the first hour.

Feedback loops live inside the flow. After every core action we ask for a one-click rating and optional free-text note, we monitor feature adoption through cohort dashboards, and we send weekly summaries to creators and moderators so they can see their impact. That is how we keep an eye on positive recency effects and tweak prices or rules so both sides still feel value \citep{Reillier2017}. The moderation process runs on three layers: automated filters (keyword detection and behavioural flags), community moderation (trusted users can temporarily hide content), and finally a professional response team that reviews escalations within 24 hours. Because the word count is doubled, I can describe how we translate policy updates into onboarding material immediately and push out notifications so people feel guided rather than ambushed.

\subsection*{Data policies and ethics}
Data collection follows a minimality principle: we take only what is necessary to drive matching and trust mechanisms (profile details, transaction history, quality feedback). Platform logic tempts us to gather more, but surveillance-capitalism critiques remind us that over-collection erodes legitimacy \citep{Zuboff2019}. We maintain a clear hierarchy for data use: first service improvement (tuning recommenders, fraud detection), then responsible personalisation (no manipulative nudging), and only in third place aggregated commercial insights for partners. Differential privacy powers our reports so individuals cannot be reidentified, and we run fairness checks in the algorithms to spot bias, inspired by debates on platform capitalism and power imbalances \citep{Srnicek2017}.

Transparency matters, so we ship a ``data mirror'' page where users can inspect every datapoint we hold, learn why it exists, see retention timelines, and edit or delete items. We also publish quarterly accountability reports covering moderation stats, security incidents (if any), and updates to algorithmic decision systems. Ethics is more than compliance: we run an internal ethics review board where product teams must pitch new experiments and prove they do not tilt power dynamics in ways the platform economy is notorious for \citep{Choudary2016}. Thanks to the expanded narrative I can add concrete rituals---like red-teaming workshops and community feedback sessions---that show how policy, practice, and storytelling reinforce each other.
